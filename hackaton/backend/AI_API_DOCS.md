# ü§ñ API d'Optimisation IA - Documentation

## Endpoint

```
POST /api/projects/:id/optimize
```

Optimise l'ordre des t√¢ches d'un projet en utilisant l'intelligence artificielle.

## Param√®tres

### Path Parameters

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `id` | string | ID du projet √† optimiser |

## R√©ponse

### Succ√®s (200 OK)

```json
{
  "success": true,
  "project": {
    "id": "1",
    "name": "Projet Airplus",
    "tasks": [
      {
        "id": "1",
        "title": "Recevoir la commande",
        "description": "R√©ception et validation",
        "status": "done",
        "department": "clients",
        "order": 0,
        "aiReasoning": "Cette t√¢che doit √™tre la premi√®re car elle initialise le processus"
      }
    ]
  },
  "optimization": {
    "dependencies": [
      {
        "from": "task-1",
        "to": "task-2",
        "type": "required",
        "reason": "La classification n√©cessite d'avoir re√ßu la commande"
      }
    ],
    "parallelGroups": [
      ["task-3", "task-4"]
    ],
    "notes": "Optimisation bas√©e sur l'analyse des d√©pendances logiques",
    "bottlenecks": [
      "Le service logistics pourrait devenir un goulot d'√©tranglement"
    ],
    "improvements": [
      "Consid√©rer la parall√©lisation des t√¢ches de validation",
      "Automatiser la classification des pi√®ces standard"
    ],
    "metadata": {
      "optimizedAt": "2025-11-26T10:30:00.000Z",
      "model": "mistral-small",
      "tasksCount": 5
    }
  }
}
```

### Erreurs

#### 404 Not Found
```json
{
  "error": "Projet non trouv√©"
}
```

#### 400 Bad Request
```json
{
  "error": "Le projet ne contient aucune t√¢che √† optimiser"
}
```

#### 500 Internal Server Error
```json
{
  "error": "Erreur lors de l'optimisation",
  "details": "Message d'erreur d√©taill√©"
}
```

## Exemple d'utilisation

### JavaScript/TypeScript

```typescript
const optimizeProject = async (projectId: string) => {
  try {
    const response = await fetch(
      `http://localhost:3001/api/projects/${projectId}/optimize`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
      }
    );

    if (!response.ok) {
      throw new Error('Erreur lors de l\'optimisation');
    }

    const data = await response.json();
    console.log('Optimisation r√©ussie:', data);
    return data;
  } catch (error) {
    console.error('Erreur:', error);
  }
};
```

### cURL

```bash
curl -X POST http://localhost:3001/api/projects/1/optimize \
  -H "Content-Type: application/json"
```

## Structure des donn√©es

### Task (T√¢che)

```typescript
interface Task {
  id: string;
  title: string;
  description: string;
  status: 'todo' | 'in-progress' | 'review' | 'done';
  department: 'clients' | 'logistics' | 'services';
  assignedTo?: string;
  dueDate?: string;
  createdAt: string;
  order: number;
  aiReasoning?: string; // Ajout√© apr√®s optimisation
}
```

### Dependency (D√©pendance)

```typescript
interface Dependency {
  from: string;        // ID de la t√¢che source
  to: string;          // ID de la t√¢che cible
  type: 'required' | 'recommended';
  reason: string;      // Explication de la d√©pendance
}
```

### OptimizationResult

```typescript
interface OptimizationResult {
  dependencies: Dependency[];
  parallelGroups: string[][];  // Groupes de t√¢ches parall√©lisables
  notes: string;               // Notes g√©n√©rales
  bottlenecks: string[];       // Goulots d'√©tranglement identifi√©s
  improvements: string[];      // Suggestions d'am√©lioration
  metadata: {
    optimizedAt: string;       // ISO date
    model: string;             // Nom du mod√®le LLM utilis√©
    tasksCount: number;        // Nombre de t√¢ches optimis√©es
  };
}
```

## Fonctionnement interne

### 1. R√©cup√©ration du projet
Le syst√®me r√©cup√®re le projet et ses t√¢ches depuis la base de donn√©es en m√©moire.

### 2. Validation
- V√©rifie que le projet existe
- V√©rifie qu'il contient au moins une t√¢che

### 3. Analyse par IA
Le service AI analyse les t√¢ches en consid√©rant :
- **Titre et description** de chaque t√¢che
- **D√©partement** assign√©
- **Statut** actuel
- **Ordre** actuel

### 4. G√©n√©ration du prompt
Un prompt structur√© est g√©n√©r√© avec :
- Liste des t√¢ches
- Contexte des d√©partements
- Instructions d'analyse
- Format de r√©ponse attendu (JSON)

### 5. Appel au LLM
Le syst√®me appelle le mod√®le LLM configur√© (Mistral, HuggingFace, etc.)

### 6. Traitement de la r√©ponse
- Parse la r√©ponse JSON
- Valide la structure
- Met √† jour l'ordre des t√¢ches
- Ajoute les m√©tadonn√©es

### 7. Fallback
En cas d'erreur avec le LLM, un algorithme heuristique prend le relais :
- Tri par d√©partement : clients ‚Üí logistics ‚Üí services
- Maintien de l'ordre relatif dans chaque d√©partement

## Configuration requise

Voir [AI_OPTIMIZATION_SETUP.md](./AI_OPTIMIZATION_SETUP.md) pour la configuration compl√®te.

### Variables d'environnement (.env)

```env
LLM_API_URL=https://api.mistral.ai/v1/chat/completions
LLM_API_KEY=your_api_key_here
LLM_MODEL=mistral-small
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000
```

## Limites et consid√©rations

### Performance
- **Temps de r√©ponse** : 2-10 secondes selon le mod√®le et le nombre de t√¢ches
- **Limite de t√¢ches** : Recommand√© jusqu'√† 50 t√¢ches par projet
- **Rate limiting** : D√©pend du provider (Mistral : ~1000 req/min)

### Co√ªt
- **Mistral Small** : ~$0.0002 par requ√™te (tr√®s √©conomique)
- **Ollama Local** : Gratuit mais n√©cessite des ressources locales
- **HuggingFace** : Gratuit avec rate limiting

### Qualit√©
La qualit√© de l'optimisation d√©pend de :
- **Descriptions des t√¢ches** : Plus elles sont d√©taill√©es, mieux c'est
- **Coh√©rence du workflow** : D√©partements et statuts bien utilis√©s
- **Mod√®le LLM** : Mod√®les plus grands = meilleure analyse

## Cas d'usage

### 1. Nouveau projet
Apr√®s avoir import√© des t√¢ches depuis un fichier JSON, optimisez pour obtenir un ordre logique.

### 2. R√©organisation
Apr√®s avoir ajout√©/modifi√© des t√¢ches, r√©optimisez pour ajuster l'ordre.

### 3. Analyse de workflow
Utilisez les suggestions d'am√©lioration pour identifier les optimisations possibles.

### 4. Documentation automatique
Les d√©pendances identifi√©es servent de documentation du workflow.

## Am√©liorations futures

- [ ] Cache des optimisations
- [ ] Historique des optimisations
- [ ] Comparaison avant/apr√®s
- [ ] Export des d√©pendances au format Gantt
- [ ] Estimation de dur√©e par l'IA
- [ ] D√©tection des risques
- [ ] Suggestions de ressources
- [ ] Optimisation multi-projets

## Support

Pour tout probl√®me ou question :
1. V√©rifiez la configuration dans `.env`
2. Consultez les logs du serveur
3. Testez avec le mode fallback (sans cl√© API)
4. V√©rifiez la documentation du provider LLM

---

Derni√®re mise √† jour : 26 novembre 2025
