/**
 * ü§ñ Service d'optimisation des t√¢ches par IA
 *
 * Ce fichier contient les configurations et exemples pour utiliser
 * diff√©rents providers LLM avec le syst√®me d'optimisation.
 */

// ============================================================================
// CONFIGURATION MISTRAL AI (Recommand√© - Simple et √©conomique)
// ============================================================================

/*
LLM_API_URL=https://api.mistral.ai/v1/chat/completions
LLM_API_KEY=votre_cl√©_mistral_ici
LLM_MODEL=mistral-small
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

AVANTAGES:
- Tr√®s rapide (2-5 secondes)
- √âconomique (~$0.0002 par requ√™te)
- Excellente qualit√© d'analyse
- API simple et stable

MOD√àLES DISPONIBLES:
- mistral-small: Rapide, √©conomique (recommand√©)
- mistral-medium: Plus puissant
- mistral-large: Maximum de performance

OBTENIR UNE CL√â:
1. Aller sur https://console.mistral.ai/
2. Cr√©er un compte
3. Aller dans "API Keys"
4. Cr√©er une nouvelle cl√©
*/

// ============================================================================
// CONFIGURATION HUGGINGFACE (Gratuit avec limitations)
// ============================================================================

/*
LLM_API_URL=https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2
LLM_API_KEY=votre_token_huggingface
LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

AVANTAGES:
- Gratuit
- Pas de limite de budget
- Plusieurs mod√®les disponibles

INCONV√âNIENTS:
- Rate limiting strict
- Peut √™tre lent (cold start)
- Moins stable que Mistral

OBTENIR UN TOKEN:
1. Aller sur https://huggingface.co/
2. Cr√©er un compte
3. Aller dans Settings > Access Tokens
4. Cr√©er un token avec permission "read"

MOD√àLES RECOMMAND√âS:
- mistralai/Mistral-7B-Instruct-v0.2
- meta-llama/Llama-2-70b-chat-hf
- codellama/CodeLlama-34b-Instruct-hf
*/

// ============================================================================
// CONFIGURATION OLLAMA (Local - Gratuit - Pas de cl√© API)
// ============================================================================

/*
LLM_API_URL=http://localhost:11434/api/generate
LLM_API_KEY=not_required
LLM_MODEL=mistral
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

AVANTAGES:
- 100% gratuit
- Pas de limite
- Confidentialit√© totale (donn√©es locales)
- Pas besoin de cl√© API

INCONV√âNIENTS:
- N√©cessite installation locale
- Utilise les ressources de votre machine
- Plus lent (5-15 secondes)

INSTALLATION:
1. T√©l√©charger Ollama: https://ollama.ai/
2. Installer l'application
3. T√©l√©charger un mod√®le:
   ollama pull mistral
   ollama pull llama2
   ollama pull codellama

4. D√©marrer Ollama (automatique au boot)

MOD√àLES DISPONIBLES:
- mistral (7B): Bon compromis
- llama2 (7B/13B/70B): Tr√®s bon
- codellama (7B/13B/34B): Sp√©cialis√© code
*/

// ============================================================================
// CONFIGURATION OPENROUTER (Acc√®s multiple mod√®les)
// ============================================================================

/*
LLM_API_URL=https://openrouter.ai/api/v1/chat/completions
LLM_API_KEY=votre_cl√©_openrouter
LLM_MODEL=mistralai/mistral-7b-instruct
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

AVANTAGES:
- Acc√®s √† de nombreux mod√®les
- Bascule facile entre mod√®les
- Prix comp√©titifs

OBTENIR UNE CL√â:
1. Aller sur https://openrouter.ai/
2. Cr√©er un compte
3. Ajouter des cr√©dits
4. Cr√©er une cl√© API

MOD√àLES POPULAIRES:
- mistralai/mistral-7b-instruct
- anthropic/claude-2
- openai/gpt-3.5-turbo
*/

// ============================================================================
// CONFIGURATION OPENAI (Alternative - Plus cher)
// ============================================================================

/*
LLM_API_URL=https://api.openai.com/v1/chat/completions
LLM_API_KEY=votre_cl√©_openai
LLM_MODEL=gpt-3.5-turbo
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

AVANTAGES:
- Tr√®s haute qualit√©
- API stable et document√©e
- Rapide

INCONV√âNIENTS:
- Plus cher (~$0.002 par requ√™te)
- N√©cessite compte OpenAI

MOD√àLES:
- gpt-3.5-turbo: Rapide et √©conomique
- gpt-4: Meilleure qualit√© mais cher
*/

// ============================================================================
// PARAM√àTRES AVANC√âS
// ============================================================================

/*
TEMP√âRATURE (LLM_TEMPERATURE)
-----------------------------
Contr√¥le la cr√©ativit√© vs d√©terminisme

0.0 - 0.2: Ultra-d√©terministe
  - R√©ponses tr√®s coh√©rentes
  - Pas de cr√©ativit√©
  - Recommand√© pour: Optimisation, analyse stricte

0.3 - 0.5: √âquilibr√© (RECOMMAND√â)
  - Bon compromis
  - Coh√©rence avec un peu de variation
  - Recommand√© pour: Usage g√©n√©ral

0.6 - 0.8: Cr√©atif
  - Plus de variation
  - Peut proposer des solutions inattendues
  - Recommand√© pour: Brainstorming, exploration

0.9 - 1.0: Tr√®s cr√©atif
  - R√©ponses impr√©visibles
  - Peut √™tre incoh√©rent
  - Pas recommand√© pour l'optimisation

MAX TOKENS (LLM_MAX_TOKENS)
---------------------------
Limite la longueur de la r√©ponse

500-1000: Pour des projets simples (< 10 t√¢ches)
1000-2000: Usage standard (RECOMMAND√â)
2000-4000: Projets complexes (> 30 t√¢ches)
4000+: Tr√®s grands projets (attention au co√ªt)

Note: Plus de tokens = co√ªt plus √©lev√©
*/

// ============================================================================
// TESTS ET D√âBOGAGE
// ============================================================================

/*
TESTER VOTRE CONFIGURATION
--------------------------

1. V√©rifier le serveur:
   curl http://localhost:3001/health

2. Tester l'optimisation:
   ./test-optimization.sh

3. Voir les logs en temps r√©el:
   npm run dev

R√âSOLUTION DE PROBL√àMES
-----------------------

Erreur "LLM_API_KEY n'est pas d√©finie":
‚û°Ô∏è Cr√©ez le fichier .env et ajoutez votre cl√©

Erreur 401 Unauthorized:
‚û°Ô∏è Votre cl√© API est invalide ou expir√©e

Erreur 429 Rate Limit:
‚û°Ô∏è Vous avez atteint la limite du provider
‚û°Ô∏è Attendez quelques minutes ou changez de provider

R√©ponse trop lente:
‚û°Ô∏è Utilisez un mod√®le plus petit (mistral-small)
‚û°Ô∏è R√©duisez LLM_MAX_TOKENS
‚û°Ô∏è Consid√©rez Ollama pour du local

Qualit√© insuffisante:
‚û°Ô∏è Augmentez LLM_MAX_TOKENS
‚û°Ô∏è Utilisez un mod√®le plus puissant
‚û°Ô∏è Am√©liorez les descriptions de t√¢ches
*/

// ============================================================================
// S√âCURIT√â ET BONNES PRATIQUES
// ============================================================================

/*
‚úÖ √Ä FAIRE:
- Utiliser des variables d'environnement
- Ne jamais commiter le fichier .env
- Limiter les permissions de la cl√© API
- Monitorer l'usage et les co√ªts
- Renouveler les cl√©s r√©guli√®rement

‚ùå √Ä √âVITER:
- Partager vos cl√©s API
- Commiter les cl√©s dans le code
- Utiliser des cl√©s de production en dev
- Laisser des cl√©s dans les logs
*/

// ============================================================================
// ESTIMATION DES CO√õTS (√† titre indicatif)
// ============================================================================

/*
PAR OPTIMISATION (10 t√¢ches):

Mistral Small:     ~$0.0002  (tr√®s √©conomique)
OpenAI GPT-3.5:    ~$0.002   (10x plus cher)
OpenAI GPT-4:      ~$0.02    (100x plus cher)
HuggingFace:       Gratuit   (rate limited)
Ollama:            Gratuit   (ressources locales)

POUR 1000 OPTIMISATIONS/MOIS:

Mistral Small:     ~$0.20
OpenAI GPT-3.5:    ~$2.00
OpenAI GPT-4:      ~$20.00
HuggingFace:       Gratuit
Ollama:            Gratuit
*/

// ============================================================================
// RECOMMANDATIONS SELON VOTRE CAS
// ============================================================================

/*
STARTUP / PETIT PROJET:
‚û°Ô∏è HuggingFace (gratuit) ou Mistral Small (tr√®s √©conomique)

D√âVELOPPEMENT LOCAL:
‚û°Ô∏è Ollama (gratuit, confidentiel, pas de d√©pendance)

PRODUCTION √Ä GRANDE √âCHELLE:
‚û°Ô∏è Mistral AI (excellent rapport qualit√©/prix/rapidit√©)

BESOIN DE QUALIT√â MAXIMALE:
‚û°Ô∏è OpenAI GPT-4 (cher mais excellent)

PROTOTYPE / POC:
‚û°Ô∏è Mode fallback (pas de cl√© requise, heuristique simple)
*/

// ============================================================================
// SUPPORT ET DOCUMENTATION
// ============================================================================

/*
LIENS UTILES:

Mistral AI:
- Docs: https://docs.mistral.ai/
- Console: https://console.mistral.ai/

HuggingFace:
- Docs: https://huggingface.co/docs/api-inference/
- Models: https://huggingface.co/models

Ollama:
- Site: https://ollama.ai/
- GitHub: https://github.com/jmorganca/ollama
- Models: https://ollama.ai/library

OpenRouter:
- Site: https://openrouter.ai/
- Docs: https://openrouter.ai/docs

FICHIERS DE DOCUMENTATION DU PROJET:
- AI_OPTIMIZATION_README.md: Guide complet
- AI_OPTIMIZATION_SETUP.md: Configuration d√©taill√©e
- backend/AI_API_DOCS.md: Documentation API
*/
