# Configuration de l'API LLM pour l'optimisation des tâches
# Exemples de providers:
# - Mistral AI: https://api.mistral.ai
# - HuggingFace Inference API: https://api-inference.huggingface.co
# - Ollama (local): http://localhost:11434
# - OpenRouter: https://openrouter.ai/api

# URL de l'API LLM
LLM_API_URL=https://api.mistral.ai/v1/chat/completions

# Clé API pour le LLM
LLM_API_KEY=your_api_key_here

# Modèle à utiliser
# Mistral: mistral-small, mistral-medium, mistral-large
# HuggingFace: meta-llama/Llama-2-70b-chat-hf, mistralai/Mistral-7B-Instruct-v0.2
LLM_MODEL=mistral-small

# Température (0.0 - 1.0, plus bas = plus déterministe)
LLM_TEMPERATURE=0.3

# Nombre maximum de tokens dans la réponse
LLM_MAX_TOKENS=2000
