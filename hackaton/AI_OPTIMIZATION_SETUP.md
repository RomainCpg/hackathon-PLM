# ü§ñ Configuration de l'Optimisation IA

Ce document explique comment configurer l'optimisation des t√¢ches par intelligence artificielle.

## üìã Vue d'ensemble

Le syst√®me d'optimisation IA analyse vos t√¢ches et sugg√®re un ordre d'ex√©cution optimal en tenant compte :
- Des d√©pendances logiques entre t√¢ches
- Des d√©partements impliqu√©s
- Des possibilit√©s de parall√©lisation
- Des goulots d'√©tranglement potentiels

## üîë Configuration de l'API LLM

### Option 1 : Mistral AI (Recommand√©)

1. **Cr√©er un compte** sur [Mistral AI](https://console.mistral.ai/)
2. **Obtenir une cl√© API** depuis le dashboard
3. **Configurer le backend** :

```bash
cd backend
cp .env.example .env
```

4. **√âditer le fichier `.env`** :

```env
LLM_API_URL=https://api.mistral.ai/v1/chat/completions
LLM_API_KEY=votre_cl√©_api_mistral
LLM_MODEL=mistral-small
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000
```

**Mod√®les disponibles :**
- `mistral-small` : Rapide et √©conomique (recommand√©)
- `mistral-medium` : Plus puissant
- `mistral-large` : Le plus performant

### Option 2 : HuggingFace Inference API

1. **Cr√©er un compte** sur [HuggingFace](https://huggingface.co/)
2. **Obtenir un token** depuis [Settings > Access Tokens](https://huggingface.co/settings/tokens)
3. **Configurer** :

```env
LLM_API_URL=https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2
LLM_API_KEY=votre_token_huggingface
LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000
```

### Option 3 : Ollama (Local - Gratuit)

1. **Installer Ollama** : [ollama.ai](https://ollama.ai/)
2. **T√©l√©charger un mod√®le** :
```bash
ollama pull mistral
```

3. **Configurer** :

```env
LLM_API_URL=http://localhost:11434/api/generate
LLM_API_KEY=not_required
LLM_MODEL=mistral
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000
```

### Option 4 : OpenRouter (Acc√®s √† plusieurs mod√®les)

1. **Cr√©er un compte** sur [OpenRouter](https://openrouter.ai/)
2. **Obtenir une cl√© API**
3. **Configurer** :

```env
LLM_API_URL=https://openrouter.ai/api/v1/chat/completions
LLM_API_KEY=votre_cl√©_openrouter
LLM_MODEL=mistralai/mistral-7b-instruct
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000
```

## üöÄ Utilisation

1. **D√©marrer le backend** :
```bash
cd backend
npm run dev
```

2. **D√©marrer le frontend** :
```bash
cd my-react-app
npm run dev
```

3. **Dans l'application** :
   - Cr√©ez ou ouvrez un projet avec des t√¢ches
   - Allez dans la vue "Diagramme Flow"
   - Cliquez sur le bouton **"ü§ñ Optimiser avec l'IA"**
   - Attendez l'analyse (quelques secondes)
   - Consultez les r√©sultats dans le panneau qui s'ouvre

## üìä R√©sultats de l'optimisation

L'IA vous fournira :

- **Ordre optimis√©** : Nouvelle s√©quence des t√¢ches
- **D√©pendances** : Relations entre les t√¢ches (requises ou recommand√©es)
- **T√¢ches parall√©lisables** : Groupes de t√¢ches pouvant √™tre ex√©cut√©es simultan√©ment
- **Goulots d'√©tranglement** : Points de blocage potentiels
- **Suggestions d'am√©lioration** : Recommandations pour optimiser le workflow

## üõ°Ô∏è Mode de secours

Si l'API LLM n'est pas configur√©e ou indisponible, le syst√®me utilise automatiquement un algorithme heuristique bas√© sur des r√®gles :
- Organisation par d√©partement (clients ‚Üí logistics ‚Üí services)
- Tri par ordre de priorit√©
- Pas d'analyse avanc√©e des d√©pendances

## üîß Param√®tres avanc√©s

### Temp√©rature (0.0 - 1.0)
- **0.0 - 0.3** : R√©ponses d√©terministes et coh√©rentes (recommand√© pour l'optimisation)
- **0.4 - 0.7** : Bon √©quilibre cr√©ativit√©/coh√©rence
- **0.8 - 1.0** : Plus cr√©atif mais moins pr√©visible

### Max Tokens
- **1000-2000** : Suffisant pour des projets moyens
- **2000-4000** : Pour des projets complexes avec beaucoup de t√¢ches

## üí° Conseils

1. **Descriptions d√©taill√©es** : Plus vos t√¢ches ont des descriptions pr√©cises, meilleure sera l'analyse
2. **D√©partements coh√©rents** : Utilisez les d√©partements de mani√®re logique
3. **T√¢ches atomiques** : Divisez les grandes t√¢ches en petites t√¢ches
4. **It√©rations** : N'h√©sitez pas √† r√©optimiser apr√®s avoir ajout√©/modifi√© des t√¢ches

## üêõ D√©pannage

### Erreur : "LLM_API_KEY n'est pas d√©finie"
‚û°Ô∏è V√©rifiez que le fichier `.env` existe dans le dossier `backend` et contient votre cl√© API

### Erreur : "Erreur API LLM (401)"
‚û°Ô∏è Votre cl√© API est invalide ou expir√©e, g√©n√©rez-en une nouvelle

### Erreur : "Format de r√©ponse LLM non reconnu"
‚û°Ô∏è V√©rifiez que l'URL de l'API et le mod√®le sont corrects pour votre provider

### L'optimisation prend trop de temps
‚û°Ô∏è Utilisez un mod√®le plus petit (`mistral-small`) ou r√©duisez `LLM_MAX_TOKENS`

## üìö Ressources

- [Documentation Mistral AI](https://docs.mistral.ai/)
- [HuggingFace Inference API](https://huggingface.co/docs/api-inference/)
- [Ollama Documentation](https://github.com/jmorganca/ollama)
- [OpenRouter Documentation](https://openrouter.ai/docs)

## üîí S√©curit√©

‚ö†Ô∏è **Important** :
- Ne commitez JAMAIS votre fichier `.env` avec vos cl√©s API
- Le fichier `.env` est d√©j√† dans `.gitignore`
- Utilisez des cl√©s API avec des permissions limit√©es
- Renouvelez vos cl√©s r√©guli√®rement

---

Pour toute question ou probl√®me, consultez la documentation de votre provider LLM ou cr√©ez une issue sur le repository.
